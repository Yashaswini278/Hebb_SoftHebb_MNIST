{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC_GHqncpaDB"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBKBwxk5pLNU",
        "outputId": "152352b1-6231-4944-ecb5-7047dac6535c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m656.0/656.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install torchlens --quiet\n",
        "!pip install rsatoolbox --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXS0VPaSpgRO"
      },
      "outputs": [],
      "source": [
        "# @title Importing dependencies\n",
        "\n",
        "from IPython.display import Image, SVG, display\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import torch\n",
        "import torchvision\n",
        "import contextlib\n",
        "import io\n",
        "import torchlens as tl\n",
        "import rsatoolbox\n",
        "from rsatoolbox.data import Dataset\n",
        "from rsatoolbox.rdm.calc import calc_rdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg-qBe-Apq6k"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import matplotlib as mpl\n",
        "mpl.set_loglevel(\"ERROR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07545499"
      },
      "outputs": [],
      "source": [
        "# @title Figure settings\n",
        "\n",
        "import ipywidgets as widgets       # interactive display\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/ClimateMatchAcademy/course-content/main/cma.mplstyle\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQsj80mrp5je"
      },
      "source": [
        "# Download and Visualize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FHoIHYDvWAfW"
      },
      "outputs": [],
      "source": [
        "# @markdown `download_mnist()`: Function to download MNIST.\n",
        "\n",
        "def download_mnist(train_prop=0.8, keep_prop=0.5):\n",
        "\n",
        "  valid_prop = 1 - train_prop\n",
        "\n",
        "  discard_prop = 1 - keep_prop\n",
        "\n",
        "  transform = torchvision.transforms.Compose(\n",
        "      [torchvision.transforms.ToTensor(),\n",
        "      torchvision.transforms.Normalize((0.1307,), (0.3081,))]\n",
        "      )\n",
        "\n",
        "\n",
        "  with contextlib.redirect_stdout(io.StringIO()): #to suppress output\n",
        "\n",
        "      full_train_set = torchvision.datasets.MNIST(\n",
        "          root=\"./data/\", train=True, download=True, transform=transform\n",
        "          )\n",
        "      full_test_set = torchvision.datasets.MNIST(\n",
        "          root=\"./data/\", train=False, download=True, transform=transform\n",
        "          )\n",
        "\n",
        "  train_set, valid_set, _ = torch.utils.data.random_split(\n",
        "      full_train_set,\n",
        "      [train_prop * keep_prop, valid_prop * keep_prop, discard_prop]\n",
        "      )\n",
        "  test_set, _ = torch.utils.data.random_split(\n",
        "      full_test_set,\n",
        "      [keep_prop, discard_prop]\n",
        "      )\n",
        "\n",
        "  print(\"Number of examples retained:\")\n",
        "  print(f\"  {len(train_set)} (training)\")\n",
        "  print(f\"  {len(valid_set)} (validation)\")\n",
        "  print(f\"  {len(test_set)} (test)\")\n",
        "\n",
        "  return train_set, valid_set, test_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqGRdTeMdckh"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a15149be-ef35-40cb-81f6-b8358fa2fe66",
        "outputId": "07d966e2-9ab7-4439-e371-65c153a0c798"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 5070920.47it/s]\n",
            "100%|██████████| 28881/28881 [00:00<00:00, 3127858.24it/s]\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 13550607.68it/s]\n",
            "100%|██████████| 4542/4542 [00:00<00:00, 2986444.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of examples retained:\n",
            "  24001 (training)\n",
            "  5999 (validation)\n",
            "  5000 (test)\n"
          ]
        }
      ],
      "source": [
        "train_set, valid_set, test_set = download_mnist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FkO3wnAC9FlN"
      },
      "outputs": [],
      "source": [
        "#@markdown To get started exploring the dataset, here are a few plotting functions:\n",
        "\n",
        "#@markdown `get_plotting_color()`: Returns a color for the specific dataset, e.g. \"train\" or model index.\n",
        "def get_plotting_color(dataset=\"train\", model_idx=None):\n",
        "  if model_idx is not None:\n",
        "    dataset = None\n",
        "\n",
        "  if model_idx == 0 or dataset == \"train\":\n",
        "    color = \"#1F77B4\" # blue\n",
        "  elif model_idx == 1 or dataset == \"valid\":\n",
        "    color = \"#FF7F0E\" # orange\n",
        "  elif model_idx == 2 or dataset == \"test\":\n",
        "    color = \"#2CA02C\" # green\n",
        "  else:\n",
        "    if model_idx is not None:\n",
        "      raise NotImplementedError(\"Colors only implemented for up to 3 models.\")\n",
        "    else:\n",
        "      raise NotImplementedError(\n",
        "          f\"{dataset} dataset not recognized. Expected 'train', 'valid' \"\n",
        "          \"or 'test'.\"\n",
        "          )\n",
        "\n",
        "  return color\n",
        "\n",
        "\n",
        "#@markdown `plot_examples(subset)`: Plot examples from the dataset organized by their predicted class\n",
        "#@markdown (if a model is provided) or by their class label otherwise\n",
        "def plot_examples(subset, num_examples_per_class=8, MLP=None, seed=None,\n",
        "                  batch_size=32, num_classes=10, ax=None):\n",
        "  \"\"\"\n",
        "  Function for visualizing example images from the dataset, organized by their\n",
        "  predicted class, if a model is provided, or by their class, otherwise.\n",
        "\n",
        "  Arguments:\n",
        "  - subset (torch dataset or torch dataset subset): dataset from which to\n",
        "    visualized images.\n",
        "  - num_examples_per_class (int, optional): number of examples to visualize per\n",
        "    class\n",
        "  - MLP (MultiLayerPerceptron or None, optional): model to use to retrieve the\n",
        "    predicted class for each image. If MLP is None, images will be organized by\n",
        "    their class label. Otherwise, images will be organized by their predicted\n",
        "    class.\n",
        "  - seed (int or None, optional): Seed to use to randomly sample images to\n",
        "    visualize.\n",
        "  - batch_size (int, optional): If MLP is not None, number of images to\n",
        "    retrieve predicted class for at one time.\n",
        "  - num_classes (int, optional): Number of classes in the data.\n",
        "  - ax (plt subplot, optional): Axis on which to plot images. If None, a new\n",
        "    axis will be created.\n",
        "\n",
        "  Returns:\n",
        "  - ax (plt subplot): Axis on which images were plotted.\n",
        "  \"\"\"\n",
        "\n",
        "  if MLP is None:\n",
        "    xlabel = \"Class\"\n",
        "  else:\n",
        "    MLP.eval()\n",
        "    xlabel = \"Predicted class\"\n",
        "\n",
        "  if ax is None:\n",
        "    fig_wid = min(8, num_classes * 0.6)\n",
        "    fig_hei = min(8, num_examples_per_class * 0.6)\n",
        "    _, ax = plt.subplots(figsize=(fig_wid, fig_hei))\n",
        "\n",
        "  if seed is None:\n",
        "    generator = None\n",
        "  else:\n",
        "    generator = torch.Generator()\n",
        "    generator.manual_seed(seed)\n",
        "\n",
        "  loader = torch.utils.data.DataLoader(\n",
        "      subset, batch_size=batch_size, shuffle=True, generator=generator\n",
        "      )\n",
        "\n",
        "  plot_images = {i: list() for i in range(num_classes)}\n",
        "  with torch.no_grad():\n",
        "    for X, y in loader:\n",
        "      if MLP is not None:\n",
        "        y = MLP(X)\n",
        "        y = torch.argmax(y, axis=1)\n",
        "\n",
        "      done = True\n",
        "      for i in range(num_classes):\n",
        "        num_to_add = int(num_examples_per_class - len(plot_images[i]))\n",
        "        if num_to_add:\n",
        "          add_images = np.where(y == i)[0]\n",
        "          if len(add_images):\n",
        "            for add_i in add_images[: num_to_add]:\n",
        "              plot_images[i].append(X[add_i, 0].numpy())\n",
        "          if len(plot_images[i]) != num_examples_per_class:\n",
        "            done = False\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "  hei, wid = X[0, 0].shape\n",
        "  final_image = np.full((num_examples_per_class * hei, num_classes * wid), np.nan)\n",
        "  for i, images in plot_images.items():\n",
        "    if len(images):\n",
        "      final_image[: len(images) * hei, i * wid: (i + 1) * wid] = np.vstack(images)\n",
        "\n",
        "  ax.imshow(final_image, cmap=\"gray\")\n",
        "\n",
        "  ax.set_xlabel(xlabel)\n",
        "  ax.set_xticks((np.arange(num_classes) + 0.5) * wid)\n",
        "  ax.set_xticklabels([f\"{int(i)}\" for i in range(num_classes)])\n",
        "  ax.set_yticks([])\n",
        "  ax.set_title(f\"Examples per {xlabel.lower()}\")\n",
        "\n",
        "  return ax\n",
        "\n",
        "#@markdown `plot_class_distribution(train_set)`: Plots the distribution of classes in each set (train, validation, test).\n",
        "def plot_class_distribution(train_set, valid_set=None, test_set=None,\n",
        "                            num_classes=10, ax=None):\n",
        "  \"\"\"\n",
        "  Function for plotting the number of examples per class in each subset.\n",
        "\n",
        "  Arguments:\n",
        "  - train_set (torch dataset or torch dataset subset): training dataset\n",
        "  - valid_set (torch dataset or torch dataset subset, optional): validation\n",
        "    dataset\n",
        "  - test_set (torch dataset or torch dataset subset, optional): test\n",
        "    dataset\n",
        "  - num_classes (int, optional): Number of classes in the data.\n",
        "  - ax (plt subplot, optional): Axis on which to plot images. If None, a new\n",
        "    axis will be created.\n",
        "\n",
        "  Returns:\n",
        "  - ax (plt subplot): Axis on which images were plotted.\n",
        "  \"\"\"\n",
        "\n",
        "  if ax is None:\n",
        "    _, ax = plt.subplots(figsize=(6, 3))\n",
        "\n",
        "  bins = np.arange(num_classes + 1) - 0.5\n",
        "\n",
        "  for dataset_name, dataset in [\n",
        "      (\"train\", train_set), (\"valid\", valid_set), (\"test\", test_set)\n",
        "      ]:\n",
        "    if dataset is None:\n",
        "      continue\n",
        "\n",
        "    if hasattr(dataset, \"dataset\"):\n",
        "      targets = dataset.dataset.targets[dataset.indices]\n",
        "    else:\n",
        "      targets = dataset.targets\n",
        "\n",
        "    outputs = ax.hist(\n",
        "        targets,\n",
        "        bins=bins,\n",
        "        alpha=0.3,\n",
        "        color=get_plotting_color(dataset_name),\n",
        "        label=dataset_name,\n",
        "        )\n",
        "\n",
        "    per_class = len(targets) / num_classes\n",
        "    ax.axhline(\n",
        "        per_class,\n",
        "        ls=\"dashed\",\n",
        "        color=get_plotting_color(dataset_name),\n",
        "        alpha=0.8\n",
        "        )\n",
        "\n",
        "  ax.set_xticks(range(num_classes))\n",
        "  ax.set_title(\"Counts per class\")\n",
        "  ax.set_xlabel(\"Class\")\n",
        "  ax.set_ylabel(\"Count\")\n",
        "  ax.legend(loc=\"center right\")\n",
        "\n",
        "  return ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC1zovRgqFAU"
      },
      "source": [
        "# Basic MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXVJwwQpqGjV"
      },
      "outputs": [],
      "source": [
        "NUM_INPUTS = np.product(train_set.dataset.data[0].shape) # size of an MNIST image\n",
        "NUM_OUTPUTS = 10 # number of MNIST classes\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Triangle(torch.nn.Module):\n",
        "    r\"\"\"Applies the Sigmoid element-wise function:\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,  power: float=1, inplace: bool = True):\n",
        "        super(Triangle, self).__init__()\n",
        "        self.inplace = inplace\n",
        "        self.power = power\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = input - torch.mean(input.data, axis=1, keepdims=True)\n",
        "        return F.relu(input, inplace=self.inplace) ** self.power\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return 'power=%s'%self.power\n",
        "\n",
        "class Softmax(torch.nn.Module):\n",
        "    r\"\"\"Applies the Sigmoid element-wise function:\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,  temperature: float=1):\n",
        "        super(Softmax, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, input):\n",
        "        #input = input - torch.mean(input.data, axis=1, keepdims=True)\n",
        "        return F.softmax(input/self.temperature, dim = 1)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return 'power=%s'%self.power\n",
        "\n",
        "class RePU(torch.nn.Module):\n",
        "    r\"\"\"Applies the Repu function element-wise:\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, power = 2, inplace: bool = False):\n",
        "        super(RePU, self).__init__()\n",
        "        self.power = power\n",
        "        self.inplace = inplace\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.relu(input, inplace=self.inplace)**self.power\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return 'power=%s'%self.power\n",
        "\n",
        "class MultiLayerPerceptron(torch.nn.Module):\n",
        "  \"\"\"\n",
        "  Simple multilayer perceptron model class with one hidden layer.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_inputs=NUM_INPUTS,\n",
        "      num_hidden=100,\n",
        "      num_outputs=NUM_OUTPUTS,\n",
        "      activation_type=\"sigmoid\",\n",
        "      temperature = None,\n",
        "      bias=False,\n",
        "      ):\n",
        "    \"\"\"\n",
        "    Initializes a multilayer perceptron with a single hidden layer.\n",
        "\n",
        "    Arguments:\n",
        "    - num_inputs (int, optional): number of input units (i.e., image size)\n",
        "    - num_hidden (int, optional): number of hidden units in the hidden layer\n",
        "    - num_outputs (int, optional): number of output units (i.e., number of\n",
        "      classes)\n",
        "    - activation_type (str, optional): type of activation to use for the hidden\n",
        "      layer ('sigmoid', 'tanh', 'relu' or 'linear')\n",
        "    - bias (bool, optional): if True, each linear layer will have biases in\n",
        "      addition to weights\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_inputs = num_inputs\n",
        "    self.num_hidden = num_hidden\n",
        "    self.num_outputs = num_outputs\n",
        "    self.activation_type = activation_type\n",
        "    self.temperature = temperature\n",
        "    self.bias = bias\n",
        "\n",
        "    # default weights (and biases, if applicable) initialization is used\n",
        "    # see https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/linear.py\n",
        "    self.lin1 = torch.nn.Linear(num_inputs, num_hidden, bias=bias)\n",
        "    self.lin2 = torch.nn.Linear(num_hidden, num_outputs, bias=bias)\n",
        "\n",
        "    self._store_initial_weights_biases()\n",
        "\n",
        "    self._set_activation() # activation on the hidden or output layer\n",
        "    self.softmax = torch.nn.Softmax(dim=1) # activation on the output layer\n",
        "\n",
        "\n",
        "  def _store_initial_weights_biases(self):\n",
        "    \"\"\"\n",
        "    Stores a copy of the network's initial weights and biases.\n",
        "    \"\"\"\n",
        "\n",
        "    self.init_lin1_weight = self.lin1.weight.data.clone()\n",
        "    self.init_lin2_weight = self.lin2.weight.data.clone()\n",
        "    if self.bias:\n",
        "      self.init_lin1_bias = self.lin1.bias.data.clone()\n",
        "      self.init_lin2_bias = self.lin2.bias.data.clone()\n",
        "\n",
        "  def _set_activation(self):\n",
        "    \"\"\"\n",
        "    Sets the activation function used for the hidden layer.\n",
        "    \"\"\"\n",
        "\n",
        "    if self.activation_type.lower() == \"sigmoid\":\n",
        "      self.activation = torch.nn.Sigmoid() # maps to [0, 1]\n",
        "    elif self.activation_type.lower() == \"tanh\":\n",
        "      self.activation = torch.nn.Tanh() # maps to [-1, 1]\n",
        "    elif self.activation_type.lower() == \"relu\":\n",
        "      self.activation = torch.nn.ReLU() # maps to positive\n",
        "    elif self.activation_type.lower() == \"identity\":\n",
        "      self.activation = torch.nn.Identity() # maps to same\n",
        "    elif self.activation_type.lower() == \"repu\":\n",
        "      self.activation = RePU()\n",
        "    elif self.activation_type.lower() == \"triangle\":\n",
        "      self.activation = Triangle()\n",
        "    elif self.activation_type.lower() == \"softmax\":\n",
        "      self.activation = Softmax(temperature = self.temperaature)\n",
        "    else:\n",
        "      raise NotImplementedError(\n",
        "          f\"{self.activation_type} activation type not recognized. Only \"\n",
        "          \"'sigmoid', 'relu' and 'identity' have been implemented so far.\"\n",
        "          )\n",
        "\n",
        "  def forward(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Runs a forward pass through the network.\n",
        "\n",
        "    Arguments:\n",
        "    - X (torch.Tensor): Batch of input images.\n",
        "    - y (torch.Tensor, optional): Batch of targets. This variable is not used\n",
        "      here. However, it may be needed for other learning rules, to it is\n",
        "      included as an argument here for compatibility.\n",
        "\n",
        "    Returns:\n",
        "    - y_pred (torch.Tensor): Predicted targets.\n",
        "    \"\"\"\n",
        "\n",
        "    h = self.activation(self.lin1(X.reshape(-1, self.num_inputs)))\n",
        "    y_pred = self.softmax(self.lin2(h))\n",
        "    return y_pred\n",
        "\n",
        "  def forward_backprop(self, X):\n",
        "    \"\"\"\n",
        "    Identical to forward(). Should not be overwritten when creating new\n",
        "    child classes to implement other learning rules, as this method is used\n",
        "    to compare the gradients calculated with other learning rules to those\n",
        "    calculated with backprop.\n",
        "    \"\"\"\n",
        "\n",
        "    h = self.activation(self.lin1(X.reshape(-1, self.num_inputs)))\n",
        "    y_pred = self.softmax(self.lin2(h))\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "  def list_parameters(self):\n",
        "    \"\"\"\n",
        "    Returns a list of model names for a gradient dictionary.\n",
        "\n",
        "    Returns:\n",
        "    - params_list (list): List of parameter names.\n",
        "    \"\"\"\n",
        "\n",
        "    params_list = list()\n",
        "\n",
        "    for layer_str in [\"lin1\", \"lin2\"]:\n",
        "      params_list.append(f\"{layer_str}_weight\")\n",
        "      if self.bias:\n",
        "        params_list.append(f\"{layer_str}_bias\")\n",
        "\n",
        "    return params_list\n",
        "\n",
        "\n",
        "  def gather_gradient_dict(self):\n",
        "    \"\"\"\n",
        "    Gathers a gradient dictionary for the model's parameters. Raises a\n",
        "    runtime error if any parameters have no gradients.\n",
        "\n",
        "    Returns:\n",
        "    - gradient_dict (dict): A dictionary of gradients for each parameter.\n",
        "    \"\"\"\n",
        "\n",
        "    params_list = self.list_parameters()\n",
        "\n",
        "    gradient_dict = dict()\n",
        "    for param_name in params_list:\n",
        "      layer_str, param_str = param_name.split(\"_\")\n",
        "      layer = getattr(self, layer_str)\n",
        "      grad = getattr(layer, param_str).grad\n",
        "      if grad is None:\n",
        "        raise RuntimeError(\"No gradient was computed\")\n",
        "      gradient_dict[param_name] = grad.detach().clone().numpy()\n",
        "\n",
        "    return gradient_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOL3k67KbIBo"
      },
      "source": [
        "# Basic Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ta8z1SubPwn"
      },
      "outputs": [],
      "source": [
        "class BasicOptimizer(torch.optim.Optimizer):\n",
        "  \"\"\"\n",
        "  Simple optimizer class based on the SGD optimizer.\n",
        "  \"\"\"\n",
        "  def __init__(self, params, lr=0.01, weight_decay=0):\n",
        "    \"\"\"\n",
        "    Initializes a basic optimizer object.\n",
        "\n",
        "    Arguments:\n",
        "    - params (generator): Generator for torch model parameters.\n",
        "    - lr (float, optional): Learning rate.\n",
        "    - weight_decay (float, optional): Weight decay.\n",
        "    \"\"\"\n",
        "\n",
        "    if lr < 0.0:\n",
        "        raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "    if weight_decay < 0.0:\n",
        "        raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
        "\n",
        "    defaults = dict(\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay,\n",
        "        )\n",
        "\n",
        "    super().__init__(params, defaults)\n",
        "\n",
        "  def step(self):\n",
        "      \"\"\"\n",
        "      Performs a single optimization step.\n",
        "      \"\"\"\n",
        "\n",
        "      for group in self.param_groups:\n",
        "        for p in group[\"params\"]:\n",
        "\n",
        "          # only update parameters with gradients\n",
        "          if p.grad is not None:\n",
        "\n",
        "            # apply weight decay to gradient, if applicable\n",
        "            if group[\"weight_decay\"] != 0:\n",
        "              p.grad = p.grad.add(p, alpha=group[\"weight_decay\"])\n",
        "\n",
        "            # apply gradient-based update\n",
        "            p.data.add_(p.grad, alpha=-group[\"lr\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onohr5KubY8q"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0mkOfQjwbasJ"
      },
      "outputs": [],
      "source": [
        "#@markdown `train_model(MLP, train_loader, valid_loader, optimizer)`: Main function.\n",
        "#@markdown Trains the model across epochs. Aggregates loss and accuracy statistics\n",
        "#@markdown from the training and validation datasets into a results dictionary which is returned.\n",
        "def train_model(MLP, train_loader, valid_loader, optimizer, num_epochs=5):\n",
        "  \"\"\"\n",
        "  Train a model for several epochs.\n",
        "\n",
        "  Arguments:\n",
        "  - MLP (torch model): Model to train.\n",
        "  - train_loader (torch dataloader): Dataloader to use to train the model.\n",
        "  - valid_loader (torch dataloader): Dataloader to use to validate the model.\n",
        "  - optimizer (torch optimizer): Optimizer to use to update the model.\n",
        "  - num_epochs (int, optional): Number of epochs to train model.\n",
        "\n",
        "  Returns:\n",
        "  - results_dict (dict): Dictionary storing results across epochs on training\n",
        "    and validation data.\n",
        "  \"\"\"\n",
        "\n",
        "  results_dict = {\n",
        "      \"avg_train_losses\": list(),\n",
        "      \"avg_valid_losses\": list(),\n",
        "      \"avg_train_accuracies\": list(),\n",
        "      \"avg_valid_accuracies\": list(),\n",
        "  }\n",
        "\n",
        "  for e in tqdm(range(num_epochs)):\n",
        "    no_train = True if e == 0 else False # to get a baseline\n",
        "    latest_epoch_results_dict = train_epoch(\n",
        "        MLP, train_loader, valid_loader, optimizer=optimizer, no_train=no_train\n",
        "        )\n",
        "\n",
        "    for key, result in latest_epoch_results_dict.items():\n",
        "      if key in results_dict.keys() and isinstance(results_dict[key], list):\n",
        "        results_dict[key].append(latest_epoch_results_dict[key])\n",
        "      else:\n",
        "        results_dict[key] = result # copy latest\n",
        "\n",
        "  return results_dict\n",
        "\n",
        "\n",
        "def train_epoch(MLP, train_loader, valid_loader, optimizer, no_train=False):\n",
        "  \"\"\"\n",
        "  Train a model for one epoch.\n",
        "\n",
        "  Arguments:\n",
        "  - MLP (torch model): Model to train.\n",
        "  - train_loader (torch dataloader): Dataloader to use to train the model.\n",
        "  - valid_loader (torch dataloader): Dataloader to use to validate the model.\n",
        "  - optimizer (torch optimizer): Optimizer to use to update the model.\n",
        "  - no_train (bool, optional): If True, the model is not trained for the\n",
        "    current epoch. Allows a baseline (chance) performance to be computed in the\n",
        "    first epoch before training starts.\n",
        "\n",
        "  Returns:\n",
        "  - epoch_results_dict (dict): Dictionary storing epoch results on training\n",
        "    and validation data.\n",
        "  \"\"\"\n",
        "\n",
        "  criterion = torch.nn.NLLLoss()\n",
        "\n",
        "  epoch_results_dict = dict()\n",
        "  for dataset in [\"train\", \"valid\"]:\n",
        "    for sub_str in [\"correct_by_class\", \"seen_by_class\"]:\n",
        "      epoch_results_dict[f\"{dataset}_{sub_str}\"] = {\n",
        "          i:0 for i in range(MLP.num_outputs)\n",
        "          }\n",
        "\n",
        "  MLP.train()\n",
        "  train_losses, train_acc = list(), list()\n",
        "  for X, y in train_loader:\n",
        "    y_pred = MLP(X, y=y)\n",
        "    #print(f'predicted shape = {y_pred.shape}')\n",
        "    #print(f'actual shape = {y.shape}')\n",
        "    loss = criterion(torch.log(y_pred), y)\n",
        "    acc = (torch.argmax(y_pred.detach(), axis=1) == y).sum() / len(y)\n",
        "    train_losses.append(loss.item() * len(y))\n",
        "    train_acc.append(acc.item() * len(y))\n",
        "    update_results_by_class_in_place(\n",
        "        y, y_pred.detach(), epoch_results_dict, dataset=\"train\",\n",
        "        num_classes=MLP.num_outputs\n",
        "        )\n",
        "    optimizer.zero_grad()\n",
        "    if not no_train:\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  num_items = len(train_loader.dataset)\n",
        "  epoch_results_dict[\"avg_train_losses\"] = np.sum(train_losses) / num_items\n",
        "  epoch_results_dict[\"avg_train_accuracies\"] = np.sum(train_acc) / num_items * 100\n",
        "\n",
        "  MLP.eval()\n",
        "  valid_losses, valid_acc = list(), list()\n",
        "  with torch.no_grad():\n",
        "    for X, y in valid_loader:\n",
        "      y_pred = MLP(X)\n",
        "      loss = criterion(torch.log(y_pred), y)\n",
        "      acc = (torch.argmax(y_pred, axis=1) == y).sum() / len(y)\n",
        "      valid_losses.append(loss.item() * len(y))\n",
        "      valid_acc.append(acc.item() * len(y))\n",
        "      update_results_by_class_in_place(\n",
        "          y, y_pred.detach(), epoch_results_dict, dataset=\"valid\"\n",
        "          )\n",
        "\n",
        "  num_items = len(valid_loader.dataset)\n",
        "  epoch_results_dict[\"avg_valid_losses\"] = np.sum(valid_losses) / num_items\n",
        "  epoch_results_dict[\"avg_valid_accuracies\"] = np.sum(valid_acc) / num_items * 100\n",
        "\n",
        "  return epoch_results_dict\n",
        "\n",
        "\n",
        "def update_results_by_class_in_place(y, y_pred, result_dict, dataset=\"train\",\n",
        "                                     num_classes=10):\n",
        "  \"\"\"\n",
        "  Updates results dictionary in place during a training epoch by adding data\n",
        "  needed to compute the accuracies for each class.\n",
        "\n",
        "  Arguments:\n",
        "  - y (torch Tensor): target labels\n",
        "  - y_pred (torch Tensor): predicted targets\n",
        "  - result_dict (dict): Dictionary storing epoch results on training\n",
        "    and validation data.\n",
        "  - dataset (str, optional): Dataset for which results are being added.\n",
        "  - num_classes (int, optional): Number of classes.\n",
        "  \"\"\"\n",
        "\n",
        "  correct_by_class = None\n",
        "  seen_by_class = None\n",
        "\n",
        "  y_pred = np.argmax(y_pred, axis=1)\n",
        "  if len(y) != len(y_pred):\n",
        "    raise RuntimeError(\"Number of predictions does not match number of targets.\")\n",
        "\n",
        "  for i in result_dict[f\"{dataset}_seen_by_class\"].keys():\n",
        "    idxs = np.where(y == int(i))[0]\n",
        "    result_dict[f\"{dataset}_seen_by_class\"][int(i)] += len(idxs)\n",
        "\n",
        "    num_correct = int(sum(y[idxs] == y_pred[idxs]))\n",
        "    result_dict[f\"{dataset}_correct_by_class\"][int(i)] += num_correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcE-YGmqbrVq"
      },
      "source": [
        "# Inspect Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1VT92VzgbtpI"
      },
      "outputs": [],
      "source": [
        "#@markdown `plot_results(results_dict)`: Plots classification losses and\n",
        "#@markdown accuracies across epochs for the training and validation sets.\n",
        "def plot_results(results_dict, num_classes=10, ax=None):\n",
        "  \"\"\"\n",
        "  Function for plotting losses and accuracies across learning.\n",
        "\n",
        "  Arguments:\n",
        "  - results_dict (dict): Dictionary storing results across epochs on training\n",
        "    and validation data.\n",
        "  - num_classes (float, optional): Number of classes, used to calculate chance\n",
        "    accuracy.\n",
        "  - ax (plt subplot, optional): Axis on which to plot results. If None, a new\n",
        "    axis will be created.\n",
        "\n",
        "  Returns:\n",
        "  - ax (plt subplot): Axis on which results were plotted.\n",
        "  \"\"\"\n",
        "\n",
        "  if ax is None:\n",
        "    _, ax = plt.subplots(figsize=(7, 3.5))\n",
        "\n",
        "  loss_ax = ax\n",
        "  acc_ax = None\n",
        "\n",
        "  chance = 100 / num_classes\n",
        "\n",
        "  plotted = False\n",
        "  for result_type in [\"losses\", \"accuracies\"]:\n",
        "    for dataset in [\"train\", \"valid\"]:\n",
        "      key = f\"avg_{dataset}_{result_type}\"\n",
        "      if key in results_dict.keys():\n",
        "        if result_type == \"losses\":\n",
        "          ylabel = \"Loss\"\n",
        "          plot_ax = loss_ax\n",
        "          ls = None\n",
        "        elif result_type == \"accuracies\":\n",
        "          if acc_ax is None:\n",
        "            acc_ax = ax.twinx()\n",
        "            acc_ax.spines[[\"right\"]].set_visible(True)\n",
        "            acc_ax.axhline(chance, ls=\"dashed\", color=\"k\", alpha=0.8)\n",
        "            acc_ax.set_ylim(-5, 105)\n",
        "          ylabel = \"Accuracy (%)\"\n",
        "          plot_ax = acc_ax\n",
        "          ls = \"dashed\"\n",
        "        else:\n",
        "          raise RuntimeError(f\"{result_type} result type not recognized.\")\n",
        "\n",
        "        data = results_dict[key]\n",
        "        plot_ax.plot(\n",
        "            data,\n",
        "            ls=ls,\n",
        "            label=dataset,\n",
        "            alpha=0.8,\n",
        "            color=get_plotting_color(dataset)\n",
        "            )\n",
        "        plot_ax.set_ylabel(ylabel)\n",
        "        plotted = True\n",
        "\n",
        "  if plotted:\n",
        "    ax.legend(loc=\"center left\")\n",
        "    ax.set_xticks(range(len(data)))\n",
        "    ax.set_xticklabels([f\"{int(e)}\" for e in range(len(data))])\n",
        "    ymin, ymax = ax.get_ylim()\n",
        "    if ymin > 0:\n",
        "      ymin = 0\n",
        "      pad = (ymax - ymin) * 0.05\n",
        "      ax.set_ylim(ymin - pad, ymax + pad)\n",
        "\n",
        "  else:\n",
        "    raise RuntimeError(\"No data found to plot.\")\n",
        "\n",
        "  ax.set_title(\"Performance across learning\")\n",
        "  ax.set_xlabel(\"Epoch\")\n",
        "\n",
        "  return ax\n",
        "\n",
        "\n",
        "#@markdown `plot_scores_per_class(results_dict)`: Plots the classification\n",
        "#@markdown accuracies by class for the training and validation sets (for the last epoch).\n",
        "def plot_scores_per_class(results_dict, num_classes=10, ax=None):\n",
        "  \"\"\"\n",
        "  Function for plotting accuracy scores for each class.\n",
        "\n",
        "  Arguments:\n",
        "  - results_dict (dict): Dictionary storing results across epochs on training\n",
        "    and validation data.\n",
        "  - num_classes (int, optional): Number of classes in the data.\n",
        "  - ax (plt subplot, optional): Axis on which to plot accuracies. If None, a new\n",
        "    axis will be created.\n",
        "\n",
        "  Returns:\n",
        "  - ax (plt subplot): Axis on which accuracies were plotted.\n",
        "  \"\"\"\n",
        "\n",
        "  if ax is None:\n",
        "    _, ax = plt.subplots(figsize=(6, 3))\n",
        "\n",
        "  avgs = list()\n",
        "  ax.set_prop_cycle(None) # reset color cycle\n",
        "  for s, dataset in enumerate([\"train\", \"valid\"]):\n",
        "    correct_by_class = results_dict[f\"{dataset}_correct_by_class\"]\n",
        "    seen_by_class = results_dict[f\"{dataset}_seen_by_class\"]\n",
        "    xs, ys = list(), list()\n",
        "    for i, total in seen_by_class.items():\n",
        "      xs.append(i + 0.3 * (s - 0.5))\n",
        "      if total == 0:\n",
        "        ys.append(np.nan)\n",
        "      else:\n",
        "        ys.append(100 * correct_by_class[i] / total)\n",
        "\n",
        "    avg_key = f\"avg_{dataset}_accuracies\"\n",
        "    if avg_key in results_dict.keys():\n",
        "      ax.axhline(\n",
        "          results_dict[avg_key][-1], ls=\"dashed\", alpha=0.8,\n",
        "          color=get_plotting_color(dataset)\n",
        "          )\n",
        "\n",
        "    ax.bar(\n",
        "        xs, ys, label=dataset, width=0.3, alpha=0.8,\n",
        "        color=get_plotting_color(dataset)\n",
        "        )\n",
        "\n",
        "  ax.set_xticks(range(num_classes))\n",
        "  ax.set_xlabel(\"Class\")\n",
        "  ax.set_ylabel(\"Accuracy (%)\")\n",
        "  ax.set_title(\"Class scores\")\n",
        "  ax.set_ylim(-5, 105)\n",
        "\n",
        "  chance = 100 / num_classes\n",
        "  ax.axhline(chance, ls=\"dashed\", color=\"k\", alpha=0.8)\n",
        "\n",
        "  ax.legend()\n",
        "\n",
        "  return ax\n",
        "\n",
        "\n",
        "#@markdown `plot_weights(MLP)`: Plots weights before and after training.\n",
        "def plot_weights(MLP, shared_colorbar=False):\n",
        "  \"\"\"\n",
        "  Function for plotting model weights and biases before and after learning.\n",
        "\n",
        "  Arguments:\n",
        "  - MLP (torch model): Model for which to plot weights and biases.\n",
        "  - shared_colorbar (bool, optional): If True, one colorbar is shared for all\n",
        "      parameters.\n",
        "\n",
        "  Returns:\n",
        "  - ax (plt subplot array): Axes on which weights and biases were plotted.\n",
        "  \"\"\"\n",
        "\n",
        "  param_names = MLP.list_parameters()\n",
        "\n",
        "  params_images = dict()\n",
        "  pre_means = dict()\n",
        "  post_means = dict()\n",
        "  vmin, vmax = np.inf, -np.inf\n",
        "  for param_name in param_names:\n",
        "    layer, param_type = param_name.split(\"_\")\n",
        "    init_params = getattr(MLP, f\"init_{layer}_{param_type}\").numpy()\n",
        "    separator = np.full((1, init_params.shape[-1]), np.nan)\n",
        "    last_params = getattr(getattr(MLP, layer), param_type).detach().numpy()\n",
        "    diff_params = last_params - init_params\n",
        "\n",
        "    params_image = np.vstack(\n",
        "        [init_params, separator, last_params, separator, diff_params]\n",
        "        )\n",
        "    vmin = min(vmin, np.nanmin(params_image))\n",
        "    vmax = min(vmax, np.nanmax(params_image))\n",
        "\n",
        "    params_images[param_name] = params_image\n",
        "    pre_means[param_name] = init_params.mean()\n",
        "    post_means[param_name] = last_params.mean()\n",
        "\n",
        "  nrows = len(param_names)\n",
        "  gridspec_kw = dict()\n",
        "  if len(param_names) == 4:\n",
        "    gridspec_kw[\"height_ratios\"] = [5, 1, 5, 1]\n",
        "    cbar_label = \"Weight/bias values\"\n",
        "  elif len(param_names) == 2:\n",
        "    gridspec_kw[\"height_ratios\"] = [5, 5]\n",
        "    cbar_label = \"Weight values\"\n",
        "  else:\n",
        "    raise NotImplementedError(\"Expected 2 parameters (weights only) or \"\n",
        "      f\"4 parameters (weights and biases), but found {len(param_names)}\"\n",
        "    )\n",
        "\n",
        "  if shared_colorbar:\n",
        "    nrows += 1\n",
        "    gridspec_kw[\"height_ratios\"].append(1)\n",
        "  else:\n",
        "    vmin, vmax = None, None\n",
        "\n",
        "  fig, axes = plt.subplots(\n",
        "      nrows, 1, figsize=(6, nrows + 3), gridspec_kw=gridspec_kw\n",
        "      )\n",
        "\n",
        "  for i, (param_name, params_image) in enumerate(params_images.items()):\n",
        "    layer, param_type = param_name.split(\"_\")\n",
        "    layer_str = \"First\" if layer == \"lin1\" else \"Second\"\n",
        "    param_str = \"weights\" if param_type == \"weight\" else \"biases\"\n",
        "\n",
        "    axes[i].set_title(f\"{layer_str} linear layer {param_str} (pre, post and diff)\")\n",
        "    im = axes[i].imshow(params_image, aspect=\"auto\", vmin=vmin, vmax=vmax)\n",
        "    if not shared_colorbar:\n",
        "      cbar = fig.colorbar(im, ax=axes[i], aspect=10)\n",
        "      cbar.ax.axhline(pre_means[param_name], ls=\"dotted\", color=\"k\", alpha=0.5)\n",
        "      cbar.ax.axhline(post_means[param_name], color=\"k\", alpha=0.5)\n",
        "\n",
        "    if param_type == \"weight\":\n",
        "      axes[i].set_xlabel(\"Input dim.\")\n",
        "      axes[i].set_ylabel(\"Output dim.\")\n",
        "    axes[i].spines[[\"left\", \"bottom\"]].set_visible(False)\n",
        "    axes[i].set_xticks([])\n",
        "    axes[i].set_yticks([])\n",
        "\n",
        "  if shared_colorbar:\n",
        "    cax = axes[-1]\n",
        "    cbar = fig.colorbar(im, cax=cax, orientation=\"horizontal\", location=\"bottom\")\n",
        "    cax.set_xlabel(cbar_label)\n",
        "\n",
        "  return axes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgkF7FOZbw5x"
      },
      "source": [
        "# Hebbian Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJNE7voam1v7"
      },
      "outputs": [],
      "source": [
        "class HebbianFunction(torch.autograd.Function):\n",
        "  \"\"\"\n",
        "  Gradient computing function class for Hebbian learning.\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def forward(context, input, weight, bias=None, nonlinearity=None, target=None):\n",
        "    \"\"\"\n",
        "    Forward pass method for the layer. Computes the output of the layer and\n",
        "    stores variables needed for the backward pass.\n",
        "\n",
        "    Arguments:\n",
        "    - context (torch context): context in which variables can be stored for\n",
        "      the backward pass.\n",
        "    - input (torch tensor): input to the layer.\n",
        "    - weight (torch tensor): layer weights.\n",
        "    - bias (torch tensor, optional): layer biases.\n",
        "    - nonlinearity (torch functional, optional): nonlinearity for the layer.\n",
        "    - target (torch tensor, optional): layer target, if applicable.\n",
        "\n",
        "    Returns:\n",
        "    - output (torch tensor): layer output.\n",
        "    \"\"\"\n",
        "\n",
        "    # compute the output for the layer (linear layer with non-linearity)\n",
        "    output = input.mm(weight.t())\n",
        "    if bias is not None:\n",
        "      output += bias.unsqueeze(0).expand_as(output)\n",
        "    if nonlinearity is not None:\n",
        "      output = nonlinearity(output)\n",
        "\n",
        "    # calculate the output to use for the backward pass\n",
        "    output_for_update = output if target is None else target\n",
        "\n",
        "    # store variables in the context for the backward pass\n",
        "    context.save_for_backward(input, weight, bias, output_for_update)\n",
        "\n",
        "    return output\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(context, grad_output=None):\n",
        "    \"\"\"\n",
        "    Backward pass method for the layer. Computes and returns the gradients for\n",
        "    all variables passed to forward (returning None if not applicable).\n",
        "\n",
        "    Arguments:\n",
        "    - context (torch context): context in which variables can be stored for\n",
        "      the backward pass.\n",
        "    - input (torch tensor): input to the layer.\n",
        "    - weight (torch tensor): layer weights.\n",
        "    - bias (torch tensor, optional): layer biases.\n",
        "    - nonlinearity (torch functional, optional): nonlinearity for the layer.\n",
        "    - target (torch tensor, optional): layer target, if applicable.\n",
        "\n",
        "    Returns:\n",
        "    - grad_input (None): gradients for the input (None, since gradients are not\n",
        "      backpropagated in Hebbian learning).\n",
        "    - grad_weight (torch tensor): gradients for the weights.\n",
        "    - grad_bias (torch tensor or None): gradients for the biases, if they aren't\n",
        "      None.\n",
        "    - grad_nonlinearity (None): gradients for the nonlinearity (None, since\n",
        "      gradients do not apply to the non-linearities).\n",
        "    - grad_target (None): gradients for the targets (None, since\n",
        "      gradients do not apply to the targets).\n",
        "    \"\"\"\n",
        "\n",
        "    input, weight, bias, output_for_update = context.saved_tensors\n",
        "    grad_input = None\n",
        "    grad_weight = None\n",
        "    grad_bias = None\n",
        "    grad_nonlinearity = None\n",
        "    grad_target = None\n",
        "\n",
        "    input_needs_grad = context.needs_input_grad[0]\n",
        "    if input_needs_grad:\n",
        "      pass\n",
        "\n",
        "    weight_needs_grad = context.needs_input_grad[1]\n",
        "    if weight_needs_grad:\n",
        "      grad_weight = output_for_update.t().mm(input)\n",
        "      grad_weight = grad_weight / len(input) # average across batch\n",
        "\n",
        "      # center around 0\n",
        "      #grad_weight = grad_weight - grad_weight.mean(axis=0) # center around 0\n",
        "\n",
        "      ## or apply Oja's rule (not compatible with clamping outputs to the targets!)\n",
        "      # oja_subtract = output_for_update.pow(2).mm(grad_weight).mean(axis=0)\n",
        "      # grad_weight = grad_weight - oja_subtract\n",
        "\n",
        "      # take the negative, as the gradient will be subtracted\n",
        "      grad_weight = -grad_weight\n",
        "\n",
        "    if bias is not None:\n",
        "      bias_needs_grad = context.needs_input_grad[2]\n",
        "      if bias_needs_grad:\n",
        "        grad_bias = output_for_update.mean(axis=0) # average across batch\n",
        "\n",
        "        # center around 0\n",
        "        grad_bias = grad_bias - grad_bias.mean()\n",
        "\n",
        "        ## or apply an adaptation of Oja's rule for biases\n",
        "        ## (not compatible with clamping outputs to the targets!)\n",
        "        # oja_subtract = (output_for_update.pow(2) * bias).mean(axis=0)\n",
        "        # grad_bias = grad_bias - oja_subtract\n",
        "\n",
        "        # take the negative, as the gradient will be subtracted\n",
        "        grad_bias = -grad_bias\n",
        "\n",
        "    return grad_input, grad_weight, grad_bias, grad_nonlinearity, grad_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoyaCYg-2As4"
      },
      "outputs": [],
      "source": [
        "class SoftHebbianFunction(torch.autograd.Function):\n",
        "  \"\"\"\n",
        "  Gradient computing function class for Soft Hebbian learning.\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def forward(context, input, weight, bias=None, softmax_temp=1, target=None):\n",
        "    # normalize\n",
        "\n",
        "    # pre activation\n",
        "    pre_act = input.mm(weight.t()) #u\n",
        "    if bias is not None:\n",
        "      pre_act += bias.unsqueeze(0).expand_as(u) #pre act\n",
        "\n",
        "    # softmax of pre activation\n",
        "    output = F.softmax(pre_act/softmax_temp, dim = 1)\n",
        "\n",
        "    # calculate the output to use for the backward pass\n",
        "    output_for_update = output if target is None else target\n",
        "\n",
        "    # store variables in the context for the backward pass\n",
        "    context.save_for_backward(input, weight, bias, pre_act, output_for_update)\n",
        "\n",
        "    return output\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(context, grad_output=None):\n",
        "\n",
        "    input, weight, bias, pre_act, output_for_update = context.saved_tensors\n",
        "    grad_input = None\n",
        "    grad_weight = None\n",
        "    grad_bias = None\n",
        "    grad_nonlinearity = None\n",
        "    grad_target = None\n",
        "\n",
        "    input_needs_grad = context.needs_input_grad[0]\n",
        "    if input_needs_grad:\n",
        "      pass\n",
        "\n",
        "    max_idx = torch.argmax(output_for_update, dim = 1)\n",
        "\n",
        "    weight_needs_grad = context.needs_input_grad[1]\n",
        "    if weight_needs_grad:\n",
        "      u_w = pre_act.unsqueeze(2) * weight.unsqueeze(0)\n",
        "      input_sum = input.sum(dim=0)\n",
        "      output_sum = output_for_update.sum(dim=0)\n",
        "      grad_weight = output_sum.unsqueeze(1) * (input_sum - u_w)\n",
        "      grad_weight = grad_weight / len(input) # average across batch\n",
        "\n",
        "\n",
        "      # Negate grad_weight only where the mask is True\n",
        "      #grad_weight = -grad_weight\n",
        "      for i in range(grad_weight.shape[0]):\n",
        "        grad_weight[i, max_idx[i], :] = -grad_weight[i, max_idx[i], :]\n",
        "\n",
        "    if bias is not None:\n",
        "      bias_needs_grad = context.needs_input_grad[2]\n",
        "      if bias_needs_grad:\n",
        "        grad_bias = output_for_update.mean(axis=0) # average across batch\n",
        "\n",
        "        # center around 0\n",
        "        grad_bias = grad_bias - grad_bias.mean()\n",
        "\n",
        "        grad_bias = -grad_bias\n",
        "\n",
        "    return grad_input, grad_weight, grad_bias, grad_nonlinearity, grad_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN9OcITEb9qf"
      },
      "outputs": [],
      "source": [
        "class HebbianMultiLayerPerceptron(MultiLayerPerceptron):\n",
        "  \"\"\"\n",
        "  Hebbian multilayer perceptron with one hidden layer.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, clamp_output=True, **kwargs):\n",
        "    \"\"\"\n",
        "    Initializes a Hebbian multilayer perceptron object\n",
        "\n",
        "    Arguments:\n",
        "    - clamp_output (bool, optional): if True, outputs are clamped to targets,\n",
        "      if available, when computing weight updates.\n",
        "    \"\"\"\n",
        "\n",
        "    self.clamp_output = clamp_output\n",
        "\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "\n",
        "  def forward(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Runs a forward pass through the network.\n",
        "\n",
        "    Arguments:\n",
        "    - X (torch.Tensor): Batch of input images.\n",
        "    - y (torch.Tensor, optional): Batch of targets, stored for the backward\n",
        "      pass to compute the gradients for the last layer.\n",
        "\n",
        "    Returns:\n",
        "    - y_pred (torch.Tensor): Predicted targets.\n",
        "    \"\"\"\n",
        "    #print(\"Applying Hebbian Update to Hidden Layer\")\n",
        "    h = HebbianFunction.apply(\n",
        "        X.reshape(-1, self.num_inputs),\n",
        "        self.lin1.weight,\n",
        "        self.lin1.bias,\n",
        "        self.activation,\n",
        "        self.temperature\n",
        "    )\n",
        "\n",
        "    # if targets are provided, they can be used instead of the last layer's\n",
        "    # output to train the last layer.\n",
        "    if y is None or not self.clamp_output:\n",
        "      targets = None\n",
        "    else:\n",
        "      targets = torch.nn.functional.one_hot(\n",
        "          y, num_classes=self.num_outputs\n",
        "          ).float()\n",
        "\n",
        "    #print(\"Applying Hebbian Update to Output Layer\")\n",
        "    y_pred = HebbianFunction.apply(\n",
        "        h,\n",
        "        self.lin2.weight,\n",
        "        self.lin2.bias,\n",
        "        self.softmax,\n",
        "        targets\n",
        "    )\n",
        "\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgViWQXKlzKT"
      },
      "outputs": [],
      "source": [
        "class SoftHebbianMultiLayerPerceptron(MultiLayerPerceptron):\n",
        "  \"\"\"\n",
        "  Hebbian multilayer perceptron with one hidden layer.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, clamp_output=True, softmax_temp = 1, **kwargs):\n",
        "    \"\"\"\n",
        "    Initializes a Hebbian multilayer perceptron object\n",
        "\n",
        "    Arguments:\n",
        "    - clamp_output (bool, optional): if True, outputs are clamped to targets,\n",
        "      if available, when computing weight updates.\n",
        "    \"\"\"\n",
        "\n",
        "    self.clamp_output = clamp_output\n",
        "    self.softmax_temp = softmax_temp\n",
        "\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "\n",
        "  def forward(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Runs a forward pass through the network.\n",
        "\n",
        "    Arguments:\n",
        "    - X (torch.Tensor): Batch of input images.\n",
        "    - y (torch.Tensor, optional): Batch of targets, stored for the backward\n",
        "      pass to compute the gradients for the last layer.\n",
        "\n",
        "    Returns:\n",
        "    - y_pred (torch.Tensor): Predicted targets.\n",
        "    \"\"\"\n",
        "    #print(\"Applying Hebbian Update to Hidden Layer\")\n",
        "    h = SoftHebbianFunction.apply(\n",
        "        X.reshape(-1, self.num_inputs),\n",
        "        self.lin1.weight,\n",
        "        self.lin1.bias,\n",
        "        self.softmax_temp,\n",
        "    )\n",
        "\n",
        "    # if targets are provided, they can be used instead of the last layer's\n",
        "    # output to train the last layer.\n",
        "    if y is None or not self.clamp_output:\n",
        "      targets = None\n",
        "    else:\n",
        "      targets = torch.nn.functional.one_hot(\n",
        "          y, num_classes=self.num_outputs\n",
        "          ).float()\n",
        "\n",
        "    #print(\"Applying Hebbian Update to Output Layer\")\n",
        "    y_pred = SoftHebbianFunction.apply(\n",
        "        h,\n",
        "        self.lin2.weight,\n",
        "        self.lin2.bias,\n",
        "        self.softmax_temp,\n",
        "        targets\n",
        "    )\n",
        "\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ThS7fbpjWt1"
      },
      "outputs": [],
      "source": [
        "class TanhSoftHebbianMultiLayerPerceptron(MultiLayerPerceptron):\n",
        "  \"\"\"\n",
        "  Hebbian multilayer perceptron with one hidden layer.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, clamp_output=True, softmax_temp = 1, **kwargs):\n",
        "    \"\"\"\n",
        "    Initializes a Hebbian multilayer perceptron object\n",
        "\n",
        "    Arguments:\n",
        "    - clamp_output (bool, optional): if True, outputs are clamped to targets,\n",
        "      if available, when computing weight updates.\n",
        "    \"\"\"\n",
        "\n",
        "    self.clamp_output = clamp_output\n",
        "    self.softmax_temp = softmax_temp\n",
        "\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "\n",
        "  def forward(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Runs a forward pass through the network.\n",
        "\n",
        "    Arguments:\n",
        "    - X (torch.Tensor): Batch of input images.\n",
        "    - y (torch.Tensor, optional): Batch of targets, stored for the backward\n",
        "      pass to compute the gradients for the last layer.\n",
        "\n",
        "    Returns:\n",
        "    - y_pred (torch.Tensor): Predicted targets.\n",
        "    \"\"\"\n",
        "    #print(\"Applying Hebbian Update to Hidden Layer\")\n",
        "    h = HebbianFunction.apply(\n",
        "        X.reshape(-1, self.num_inputs),\n",
        "        self.lin1.weight,\n",
        "        self.lin1.bias,\n",
        "        self.activation\n",
        "    )\n",
        "\n",
        "    # if targets are provided, they can be used instead of the last layer's\n",
        "    # output to train the last layer.\n",
        "    if y is None or not self.clamp_output:\n",
        "      targets = None\n",
        "    else:\n",
        "      targets = torch.nn.functional.one_hot(\n",
        "          y, num_classes=self.num_outputs\n",
        "          ).float()\n",
        "\n",
        "    #print(\"Applying Hebbian Update to Output Layer\")\n",
        "    y_pred = SoftHebbianFunction.apply(\n",
        "        h,\n",
        "        self.lin2.weight,\n",
        "        self.lin2.bias,\n",
        "        self.softmax_temp,\n",
        "        targets\n",
        "    )\n",
        "\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHBKaxk-cF3x"
      },
      "source": [
        "# Simplify Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wUbJaLxi5hIF"
      },
      "outputs": [],
      "source": [
        "#@markdown The following function returns a dataset restricted to specific classes:\n",
        "\n",
        "#@markdown `restrict_classes(dataset)`: Keeps specified classes in a dataset.\n",
        "\n",
        "def restrict_classes(dataset, classes=[6], keep=True):\n",
        "  \"\"\"\n",
        "  Removes or keeps specified classes in a dataset.\n",
        "\n",
        "  Arguments:\n",
        "  - dataset (torch dataset or subset): Dataset with class targets.\n",
        "  - classes (list): List of classes to keep or remove.\n",
        "  - keep (bool): If True, the classes specified are kept. If False, they are\n",
        "  removed.\n",
        "\n",
        "  Returns:\n",
        "  - new_dataset (torch dataset or subset): Datset restricted as specified.\n",
        "  \"\"\"\n",
        "\n",
        "  if hasattr(dataset, \"dataset\"):\n",
        "    indices = np.asarray(dataset.indices)\n",
        "    targets = dataset.dataset.targets[indices]\n",
        "    dataset = dataset.dataset\n",
        "  else:\n",
        "    indices = np.arange(len(dataset))\n",
        "    targets = dataset.targets\n",
        "\n",
        "  specified_idxs = np.isin(targets, np.asarray(classes))\n",
        "  if keep:\n",
        "    retain_indices = indices[specified_idxs]\n",
        "  else:\n",
        "    retain_indices = indices[~specified_idxs]\n",
        "\n",
        "  new_dataset = torch.utils.data.Subset(dataset, retain_indices)\n",
        "\n",
        "  return new_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "-Mk7rPb06VTu",
        "outputId": "e5366ee6-dae6-422c-e0fd-0dcdef273249"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 600x300 with 1 Axes>"
            ]
          },
          "metadata": {
            "image/png": {
              "height": 275,
              "width": 575
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_set_new = restrict_classes(train_set, [0, 1])\n",
        "valid_set_new = restrict_classes(valid_set, [0, 1])\n",
        "\n",
        "plot_class_distribution(train_set_new, valid_set_new)\n",
        "\n",
        "train_loader_new = torch.utils.data.DataLoader(train_set_new, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader_new = torch.utils.data.DataLoader(valid_set_new, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxA9-zGicqbb"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efkbPBjjcsRo"
      },
      "outputs": [],
      "source": [
        "NUM_HIDDEN = 100\n",
        "HEBB_LR = 1e-6\n",
        "\n",
        "HebbianMLP = TanhSoftHebbianMultiLayerPerceptron(\n",
        "    num_hidden=NUM_HIDDEN,\n",
        "    num_outputs=2,\n",
        "    clamp_output=True,\n",
        "    softmax_temp = 0.01,\n",
        "    activation_type=\"tanh\"\n",
        ")\n",
        "\n",
        "Hebb_optimizer = BasicOptimizer(HebbianMLP.parameters(), lr=HEBB_LR)\n",
        "\n",
        "Hebb_results_dict = train_model(\n",
        "    HebbianMLP,\n",
        "    train_loader_new,\n",
        "    valid_loader_new,\n",
        "    Hebb_optimizer,\n",
        "    num_epochs=10\n",
        "    );\n",
        "\n",
        "plot_results(Hebb_results_dict, num_classes=2)\n",
        "plot_scores_per_class(Hebb_results_dict, num_classes=2)\n",
        "plot_examples(valid_loader_new.dataset, MLP=HebbianMLP, num_classes=2)\n",
        "plot_weights(HebbianMLP);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr5wwjh1dmEK",
        "outputId": "da6502f2-e4a4-4b74-904e-5c226dff79df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'avg_train_losses': [11.298114305725335, 11.775495582735296, inf, inf, inf, inf, inf, inf, inf, inf], 'avg_valid_losses': [10.513197619140772, 11.407151344416125, 12.683666412144492, 14.804633334987496, inf, inf, inf, inf, inf, inf], 'avg_train_accuracies': [53.958658346333856, 53.958658346333856, 54.36817472698908, 54.11466458658346, 53.97815912636506, 53.25663026521062, 53.2176287051482, 53.10062402496099, 53.29563182527301, 53.315132605304214], 'avg_valid_accuracies': [55.35858176726664, 56.16438354003055, 56.56728440960216, 56.80902494143133, 56.2449637004966, 56.325543920999564, 56.16438356644679, 55.68090250278844, 55.68090250278844, 55.68090250278844], 'train_correct_by_class': {0: 22, 1: 2712}, 'train_seen_by_class': {0: 2404, 1: 2724}, 'valid_correct_by_class': {0: 1, 1: 690}, 'valid_seen_by_class': {0: 551, 1: 690}}\n"
          ]
        }
      ],
      "source": [
        "print(Hebb_results_dict)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pC_GHqncpaDB",
        "MOL3k67KbIBo"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}